{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/capitanalegria/MUGSHOTGAN/blob/main/MUGSHOT_GAN_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_sHBJLwE6-s"
      },
      "source": [
        "# MUGSHOT_GAN set up libraries, modules, mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9eOG896ry0H"
      },
      "outputs": [],
      "source": [
        "#@title Mount google drive, create main folders (mugshotGAN, GoogleImageDL, jojo, Real-ESRGAN, pix2pix, RESULTS). { display-mode: \"form\" }\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "\n",
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install tqdm gdown scikit-learn==0.22 scipy lpips dlib opencv-python wandb\n",
        "!nvidia-smi\n",
        "!pip install opencv-contrib-python --upgrade\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "work_folder='/content/drive/MyDrive/'\n",
        "mugshotGAN_folder=work_folder+'mugshotGAN/'\n",
        "GoogleImageDL=mugshotGAN_folder+'GoogleImageDL/'\n",
        "RealESRGAN_folder=mugshotGAN_folder+'realESRGAN/'\n",
        "JoJo_folder=mugshotGAN_folder+'JoJo/'\n",
        "pix2pix_folder=mugshotGAN_folder+'pix2pix/'\n",
        "RESULTS=mugshotGAN_folder+'RESULTS/'\n",
        "\n",
        "\n",
        "os.makedirs(mugshotGAN_folder, exist_ok=True)\n",
        "os.makedirs(GoogleImageDL, exist_ok=True)\n",
        "os.makedirs(JoJo_folder, exist_ok=True)\n",
        "os.makedirs(RealESRGAN_folder, exist_ok=True)\n",
        "os.makedirs(pix2pix_folder, exist_ok=True)\n",
        "os.makedirs(RESULTS, exist_ok=True)\n",
        "\n",
        "%cd {mugshotGAN_folder}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xh36i4iNs_3N"
      },
      "outputs": [],
      "source": [
        "#@title Load JoJoGAN from github repository.\n",
        "!git clone https://github.com/mchong6/JoJoGAN.git {JoJo_folder}\n",
        "%cd {JoJo_folder}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F50ju05EgX_B"
      },
      "outputs": [],
      "source": [
        "#@title Setup JoJoGAN and import essential libraries. { form-width: \"100px\", display-mode: \"form\" }\n",
        "#@markdown This will take a few minutes.\n",
        "!wget -nc https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import dlib\n",
        "import imutils\n",
        "from imutils import face_utils\n",
        "from imutils.video import count_frames\n",
        "from pathlib import Path\n",
        "from scipy import ndimage\n",
        "from matplotlib import pyplot as plt\n",
        "import imageio as io\n",
        "\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark = True\n",
        "from torchvision import transforms, utils\n",
        "from util import *\n",
        "from PIL import Image\n",
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from torch import nn, autograd, optim\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "from model import *\n",
        "from e4e_projection import projection as e4e_projection\n",
        "\n",
        "from google.colab import files\n",
        "from copy import deepcopy\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "os.makedirs('inversion_codes', exist_ok=True)\n",
        "os.makedirs('style_images', exist_ok=True)\n",
        "os.makedirs('style_images_aligned', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tkkXNhVqwNCR"
      },
      "outputs": [],
      "source": [
        "#@title Create Folder Maker function, loop thru folder function as List Loader, and load MODULES folder.\n",
        "#folder function===================================================\n",
        "def foldermaker(folder_name=''):\n",
        "        if os.path.exists(folder_name):\n",
        "            print('folder {}'.format(folder_name)+' READY!')\n",
        "        else:\n",
        "            os.mkdir('{}'.format(folder_name))\n",
        "            print('folder {}'.format(folder_name)+' READY!')\n",
        "\n",
        "#load contents of a folder and pass them during looping functions\n",
        "def list_loader(folder):\n",
        "    folder_array=os.listdir(folder)\n",
        "    total_files=len(folder_array)\n",
        "    return (folder_array, total_files)\n",
        "\n",
        "#main folders ====================================================\n",
        "MODELS='{}'.format(mugshotGAN_folder)+'MODELS/'\n",
        "foldermaker(MODELS)\n",
        "JoJoMODELS=JoJo_folder+'models/'\n",
        "foldermaker(JoJoMODELS)\n",
        "\n",
        "#import modules =================================================\n",
        "modules=mugshotGAN_folder+'MODULES/'\n",
        "foldermaker(modules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Pe5AmpOKwi-9"
      },
      "outputs": [],
      "source": [
        "#@title Set up Google Image DL.\n",
        "#@markdown \n",
        "#@markdown\n",
        "\n",
        "##@markdown What image do you want to download:\n",
        "##@markdown\n",
        "\n",
        "#search_image = 'Mugshot' #@param {type: 'string'}\n",
        "#search_image=search_image.replace(' ', '_') \n",
        "#image_folder='{}'.format(RESULTS)+str(search_image)+'/'\n",
        "\n",
        "\n",
        "#enter face you want ============================================\n",
        "#search_image=input('what image do you want to search? ')\n",
        "%cd {work_folder}\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "#install google image download\n",
        "!git clone https://github.com/Joeclinton1/google-images-download.git {GoogleImageDL}\n",
        "%cd {GoogleImageDL} \n",
        "!ls\n",
        "!sudo python setup.py install\n",
        "\n",
        "#torch.save(search_image, f'{mugshotGAN_folder}search_image.torch', _use_new_zipfile_serialization=False)\n",
        "##@markdown\n",
        "##@markdown Download images or set variable?\n",
        "#download_images = \"Download\" #@param [\"Download\", \"Set variable\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4imisTxBburL"
      },
      "source": [
        "# SKIP!!!! if you want to generate digital MUGSHOTS  \n",
        "#================================\n",
        "OR Download images and train your OWN pix2pix BY RUNNING THIS SECTION.\n",
        "\n",
        "\n",
        "(skip this step if you are using the pre-trained checkpoint mugshotGAN_4, included below from mint.tokyo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s3rFy2t7P-H-"
      },
      "outputs": [],
      "source": [
        "#@title Load search image variable.\n",
        "#@markdown \n",
        "#@markdown What image do you want to download:\n",
        "#@markdown \n",
        "search_image = 'Mugshot' #@param {type: 'string'}\n",
        "search_image=search_image.replace(' ', '_') \n",
        "\n",
        "#search_image=torch.load(f'{mugshotGAN_folder}search_image.torch')\n",
        "image_folder='{}'.format(RESULTS)+str(search_image)+'/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hzicSMYUJ6LV"
      },
      "outputs": [],
      "source": [
        "#@title Download images.\n",
        "!googleimagesdownload -k {search_image} --chromedriver /usr/bin/chromedriver -ri -l 2000 -o {RESULTS} -s large -f jpg\n",
        "\n",
        "%cd {mugshotGAN_folder}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AgfLsIELgsLq"
      },
      "outputs": [],
      "source": [
        "#@title Download and/or load JoJoGAN essential models\n",
        "#@markdown You may optionally enable downloads with pydrive in order to authenticate and avoid drive download limits. DOWNLOAD WITH PYDRIVE ON YOUR FIRST RUN.\n",
        "#\n",
        "#@markdown Defaults to downloaded models. SMASH THAT GO BUTTON!\n",
        "%cd {JoJo_folder}\n",
        "\n",
        "download_with_pydrive = True #@param {type:\"boolean\"}    \n",
        "device = 'cuda' #@param ['cuda', 'cpu']\n",
        "\n",
        "!wget -nc http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "!mv shape_predictor_68_face_landmarks.dat models/dlibshape_predictor_68_face_landmarks.dat\n",
        "#!cp {JoJoMODELS}shape_predictor_68_face_landmarks.dat models/dlibshape_predictor_68_face_landmarks.dat\n",
        "#!cp {JoJoMODELS}stylegan2-ffhq-config-f.pt models/stylegan2-ffhq-config-f.pt\n",
        "#!cp {JoJoMODELS}e4e_ffhq_encode.pt models/e4e_ffhq_encode.pt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "drive_ids = {\n",
        "    \"stylegan2-ffhq-config-f.pt\": \"1Yr7KuD959btpmcKGAUsbAk5rPjX2MytK\",\n",
        "    \"e4e_ffhq_encode.pt\": \"1o6ijA3PkcewZvwJJ73dJ0fxhndn0nnh7\",\n",
        "    \"restyle_psp_ffhq_encode.pt\": \"1nbxCIVw9H3YnQsoIPykNEFwWJnHVHlVd\",\n",
        "    \"arcane_caitlyn.pt\": \"1gOsDTiTPcENiFOrhmkkxJcTURykW1dRc\",\n",
        "    \"arcane_caitlyn_preserve_color.pt\": \"1cUTyjU-q98P75a8THCaO545RTwpVV-aH\",\n",
        "    \"arcane_jinx_preserve_color.pt\": \"1jElwHxaYPod5Itdy18izJk49K1nl4ney\",\n",
        "    \"arcane_jinx.pt\": \"1quQ8vPjYpUiXM4k1_KIwP4EccOefPpG_\",\n",
        "    \"arcane_multi_preserve_color.pt\": \"1enJgrC08NpWpx2XGBmLt1laimjpGCyfl\",\n",
        "    \"arcane_multi.pt\": \"15V9s09sgaw-zhKp116VHigf5FowAy43f\",\n",
        "    \"sketch_multi.pt\": \"1GdaeHGBGjBAFsWipTL0y-ssUiAqk8AxD\",\n",
        "    \"disney.pt\": \"1zbE2upakFUAx8ximYnLofFwfT8MilqJA\",\n",
        "    \"disney_preserve_color.pt\": \"1Bnh02DjfvN_Wm8c4JdOiNV4q9J7Z_tsi\",\n",
        "    \"jojo.pt\": \"13cR2xjIBj8Ga5jMO7gtxzIJj2PDsBYK4\",\n",
        "    \"jojo_preserve_color.pt\": \"1ZRwYLRytCEKi__eT2Zxv1IlV6BGVQ_K2\",\n",
        "    \"jojo_yasuho.pt\": \"1grZT3Gz1DLzFoJchAmoj3LoM9ew9ROX_\",\n",
        "    \"jojo_yasuho_preserve_color.pt\": \"1SKBu1h0iRNyeKBnya_3BBmLr4pkPeg_L\",\n",
        "    \"art.pt\": \"1a0QDEHwXQ6hE_FcYEyNMuv5r5UnRQLKT\",\n",
        "}\n",
        "\n",
        "# from StyelGAN-NADA\n",
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "        \n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "    \n",
        "    def download_file(self, file_name):\n",
        "        file_dst = os.path.join('models', file_name)\n",
        "        file_id = drive_ids[file_name]\n",
        "        if not os.path.exists(file_dst):\n",
        "            print(f'Downloading {file_name}')\n",
        "            if self.use_pydrive:\n",
        "                downloaded = self.drive.CreateFile({'id':file_id})\n",
        "                downloaded.FetchMetadata(fetch_all=True)\n",
        "                downloaded.GetContentFile(file_dst)\n",
        "            else:\n",
        "                !gdown --id $file_id -O $file_dst\n",
        "\n",
        "if download_with_pydrive == True:\n",
        "    downloader = Downloader(download_with_pydrive)\n",
        "    downloader.download_file('stylegan2-ffhq-config-f.pt')\n",
        "    downloader.download_file('e4e_ffhq_encode.pt')\n",
        "\n",
        "latent_dim = 512\n",
        "\n",
        "# Load original generator\n",
        "original_generator = Generator(1024, latent_dim, 8, 2).to(device)\n",
        "ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
        "original_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
        "mean_latent = original_generator.mean_latent(10000)\n",
        "\n",
        "# to be finetuned generator\n",
        "generator = deepcopy(original_generator)\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((1024, 1024)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JCo2grv_4Uib"
      },
      "outputs": [],
      "source": [
        "#@title Run JoJo face detect, resize and crop faces.\n",
        "#@markdown Choose what kind of output for the JoJoGAN face detect you want:\n",
        "jojo_facedetect_output = \"jpgs_and_variables\" #@param [\"jpgs_and_variables\", \"only_variables\"]\n",
        "\n",
        "#@markdown This cell autosaves a pytorch checkpoint var with the results to reuse in case of crash.\n",
        "\n",
        "def jojo_cuda_DetectResize(folder=image_folder, search_image=search_image):\n",
        "\n",
        "#cuda as device ===================================================================\n",
        "    device = 'cuda'\n",
        "\n",
        " #result folder n variable ========================================================   \n",
        "    FACES=RESULTS+search_image+'_FACES/'\n",
        "    foldermaker(FACES)\n",
        "    aligned_faces=[]\n",
        "    my_ws=[]\n",
        "\n",
        "#load folder======================================================================\n",
        "    filepath, total_files=list_loader(folder)\n",
        "\n",
        "#loop thru folder=================================================================\n",
        "    for f in range (0,total_files):\n",
        "        img=folder+filepath[f]\n",
        "        #print(img)\n",
        "\n",
        "#skip folders n weird formats ===================================================\n",
        "        img_extensions=('.jpg', '.png')\n",
        "        if img.endswith(img_extensions):\n",
        "            try:\n",
        "                plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# filepath = list(uploaded.keys())[0]\n",
        "                name = strip_path_extension(img)+'.pt'\n",
        "\n",
        "# aligns and crops face =====================\n",
        "                aligned_face= align_face(img)\n",
        "                aligned_face.save(FACES+'{}_JOJOface_{}.jpg'.format(search_image, f), \"JPEG\", quality=100, optimize=True, progressive=True)\n",
        "                my_w = e4e_projection(aligned_face, name, device).unsqueeze(0)\n",
        "                #print(aligned_face)\n",
        "                #display_image(aligned_face, title='Aligned face')\n",
        "                aligned_faces.append(aligned_face)\n",
        "                my_ws.append(my_w)\n",
        "                f+=1\n",
        "            except Exception:\n",
        "                pass\n",
        "    return (FACES, device, aligned_faces, my_ws)\n",
        "\n",
        "\n",
        "def jojo_cuda_DetectResize_nosave(folder=image_folder):\n",
        "\n",
        "#cuda as device ===================================================================\n",
        "    device = 'cuda'\n",
        "\n",
        " #result folder n variable ========================================================   \n",
        "    FACES=RESULTS+search_image+'_FACES/'\n",
        "    foldermaker(FACES)\n",
        "    aligned_faces=[]\n",
        "    my_ws=[]\n",
        "\n",
        "#load folder======================================================================\n",
        "    filepath, total_files=list_loader(folder)\n",
        "\n",
        "#loop thru folder=================================================================\n",
        "    for f in range (0,total_files):\n",
        "        img=folder+filepath[f]\n",
        "        #print(img)\n",
        "\n",
        "#skip folders n weird formats ===================================================\n",
        "        img_extensions=('.jpg', '.png')\n",
        "        if img.endswith(img_extensions):\n",
        "            try:\n",
        "                plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# filepath = list(uploaded.keys())[0]\n",
        "                name = strip_path_extension(img)+'.pt'\n",
        "\n",
        "# aligns and crops face =====================\n",
        "                aligned_face= align_face(img)\n",
        "                #aligned_face.save(FACES+'{}_JOJOface_{}.jpg'.format(search_image, f), \"JPEG\", quality=100, optimize=True, progressive=True)\n",
        "\n",
        "                # my_w = restyle_projection(aligned_face, name, device, n_iters=1).unsqueeze(0)\n",
        "                my_w = e4e_projection(aligned_face, name, device).unsqueeze(0)\n",
        "\n",
        "                #print(aligned_face)\n",
        "                #display_image(aligned_face, title='Aligned face')\n",
        "                my_ws.append(my_w)\n",
        "                aligned_faces.append(aligned_face)\n",
        "                f+=1\n",
        "            except Exception:\n",
        "                pass\n",
        "    return (FACES, device, aligned_faces, my_ws)\n",
        "run_after_pix2pix = False #@param {type:\"boolean\"}\n",
        "if run_after_pix2pix == True:\n",
        "    image_folder_nu=RealESRGAN_result_folder\n",
        "if jojo_facedetect_output == \"jpgs_and_variables\":\n",
        "    FACES, device, aligned_faces, my_ws=jojo_cuda_DetectResize(image_folder, search_image)\n",
        "    torch.save(aligned_faces, f'{mugshotGAN_folder}{search_image}_aligned_faces.torch', _use_new_zipfile_serialization=False)\n",
        "    torch.save(my_ws, f'{JoJo_folder}{search_image}_my_ws.torch', _use_new_zipfile_serialization=False)\n",
        "else:\n",
        "    FACES, device, aligned_faces, my_ws=jojo_cuda_DetectResize_nosave(image_folder)\n",
        "    torch.save(aligned_faces, f'{mugshotGAN_folder}{search_image}_aligned_faces.torch', _use_new_zipfile_serialization=False)\n",
        "    torch.save(my_ws, f'{JoJo_folder}{search_image}_my_ws.torch', _use_new_zipfile_serialization=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2VMR6lYQ7l49"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to reload pytorch checkpoints for jojo face variables in case of crash.\n",
        "#@markdown You can skip if this is first run.\n",
        "my_ws=torch.load(f'{mugshotGAN_folder}{search_image}_my_ws.torch')\n",
        "aligned_faces=torch.load(f'{mugshotGAN_folder}{search_image}_aligned_faces.torch')\n",
        "FACES=RESULTS+'{}_FACES/'.format(search_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlyUM72sND4q"
      },
      "outputs": [],
      "source": [
        "#@title Optionally transfer all images from multiple searches into one folder. { display-mode: \"form\" }\n",
        "if search_image_nu != None:\n",
        "    mega_image_folder=RESULTS+'{}_mega_folder/'.format(search_image_nu)\n",
        "else:\n",
        "    mega_image_folder=RESULTS+'{}_mega_folder/'.format(search_image)\n",
        "foldermaker(mega_image_folder)\n",
        "#@markdown Enter folder to copy.\n",
        "face_folder='/content/drive/MyDrive/mugshotGAN/pix2pix/results/mugshotGAN_4/test_latest/images'#@param\n",
        "!rsync -a {face_folder} {mega_image_folder}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FE0U2yIrIAgF"
      },
      "outputs": [],
      "source": [
        "#@title Set up Real-ESRGAN to upscale the image DATASET.\n",
        "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git {RealESRGAN_folder}\n",
        "%cd {RealESRGAN_folder}\n",
        "# Set up the environment\n",
        "!pip install basicsr\n",
        "!pip install facexlib\n",
        "!pip install gfpgan\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "\n",
        "# Download the pre-trained model\n",
        "!wget -nc https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
        "\n",
        "import shutil\n",
        "\n",
        "os.makedirs('temp_inputs', exist_ok=True) \n",
        "FACES=RESULTS+'{}_FACES/'.format(search_image)\n",
        "RealESRGAN_load_folder = FACES\n",
        "RealESERGAN_upload_folder=RealESRGAN_folder+'temp_inputs/'\n",
        "#!rsync -a {RealESRGAN_load_folder} {RealESRGAN_upload_folder}\n",
        "if search_image_nu != None:\n",
        "    fake_folder=RESULTS+search_image_nu+'_FAKE/'\n",
        "    RealESRGAN_result_folder = RESULTS+search_image_nu+'_ESRGAN/'    \n",
        "else:\n",
        "    RealESRGAN_result_folder = RESULTS+search_image+'_ESRGAN/'\n",
        "foldermaker(RealESRGAN_result_folder)\n",
        "realERSGAN_input = fake_folder #@param [\"mega_image_folder\", \"FACES\", \"image_folder\", \"None\", \"face_folder\", \"fake_folder\"] {type:\"raw\"}\n",
        "!python inference_realesrgan.py -n RealESRGAN_x4plus --input {realERSGAN_input} --outscale 3.5 --face_enhance --output {RealESRGAN_result_folder}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zMh9aqELzFw"
      },
      "outputs": [],
      "source": [
        "#@title Create train data: { form-width: \"200px\", display-mode: \"form\" }\n",
        "#@markdown Define Face Crop function and crop faces. Resize and split into folders for pix2pix GAN.\n",
        "\n",
        "#!cp {modules}FACE_CROPPER.py .\n",
        "#from FACE_CROPPER import FACE_CROPPER\n",
        "work_folder='/content/drive/MyDrive/'\n",
        "\n",
        "#RealESRGAN_result_folder=RESULTS+search_image+'_ESRGAN/'\n",
        "\n",
        "FACES=RESULTS+search_image+'_FACES/'\n",
        "RealESRGAN_result_folder=FACES\n",
        "\n",
        "#@markdown Select size options:\n",
        "width_and_height = 1024 #@param [\"256\", \"512\", \"1024\"] {type:\"raw\"}\n",
        "\n",
        "import cv2\n",
        "import dlib\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def FACE_CROPPER (folder, width=width_and_height, height=width_and_height):\n",
        "    croppedFACES=RESULTS+'{}_crop/'.format(search_image)\n",
        "    foldermaker(croppedFACES)\n",
        "\n",
        "    A_folder=croppedFACES+'A/'\n",
        "    B_folder=croppedFACES+'B/'\n",
        "    foldermaker(A_folder)\n",
        "    foldermaker(B_folder)\n",
        "\n",
        "    A_train=A_folder+'train/'\n",
        "    B_train=B_folder+'train/'\n",
        "    foldermaker(A_train)\n",
        "    foldermaker(B_train)\n",
        "\n",
        "    A_val=A_folder+'val/'\n",
        "    B_val=B_folder+'val/'\n",
        "    foldermaker(A_val)\n",
        "    foldermaker(B_val)\n",
        "    \n",
        "    A_test=A_folder+'test/'\n",
        "    B_test=B_folder+'test/'\n",
        "    foldermaker(A_test)\n",
        "    foldermaker(B_test)\n",
        "    \n",
        "    dim=(width, height)\n",
        "\n",
        "#model ================================================================\n",
        "    detector=dlib.get_frontal_face_detector()\n",
        "    model=dlib.shape_predictor(JoJo_folder+'models/dlibshape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "#source=================================================================\n",
        "    FACES, frameCOUNT=list_loader(folder)\n",
        "\n",
        "#loop trhu folder ======================================================\n",
        "    for f in range(0,frameCOUNT):\n",
        "        print(f)\n",
        "\n",
        "#string path ===========================================================\n",
        "        imash=folder+FACES[f]\n",
        "        img_extensions=('.jpg','.png')\n",
        "\n",
        "#only JPGS AND PNGS  ===================================================\n",
        "        if imash.endswith(img_extensions):\n",
        "            try:\n",
        "\n",
        "#DLIB LOAD==============================================================\n",
        "                IMG=dlib.load_rgb_image(imash)   \n",
        "#CV LOAD  ==============================================================\n",
        "                #IMG = cv2.imread(imash)[:,:,::-1]\n",
        "            \n",
        "#find face =============================================================\n",
        "                FACE=detector(IMG, 1)\n",
        "            \n",
        "#Pass2landmark =========================================================\n",
        "                landmark_array=[]\n",
        "                for k, d, in enumerate(FACE):\n",
        "                    landmarks=model(IMG, d)\n",
        "\n",
        "#select only landmarks 0-27 to cutout face=============================\n",
        "                    for n in range(0,27):\n",
        "                        x=landmarks.part(n).x\n",
        "                        y=landmarks.part(n).y\n",
        "                        landmark_array.append((x, y))\n",
        "                        #cv2.circle(IMG, (x,y), 2, (255, 255, 0), -1)\n",
        "            \n",
        " #detect if face is frontal (\"manually\")\n",
        "                frontson1=landmark_array[0][1] \n",
        "                frontson2=landmark_array[17][1]\n",
        "\n",
        "                cutout_guide=[]\n",
        "\n",
        "#outline path from landmarks 0-27 ===================================\n",
        "                for i in range (15, -1, -1):\n",
        "                    from_coordinate=landmark_array[i+1]\n",
        "                    to_coordinate=landmark_array[i]\n",
        "                    cutout_guide.append(from_coordinate)\n",
        "            \n",
        "                from_coordinate=landmark_array[0]\n",
        "                to_coordinate=landmark_array[17]\n",
        "                cutout_guide.append(from_coordinate)\n",
        "\n",
        "                for i in range (17, 20):\n",
        "                    from_coordinate=landmark_array[i]\n",
        "                    to_coordinate=landmark_array[i+1]\n",
        "                    cutout_guide.append(from_coordinate)\n",
        "            \n",
        "                from_coordinate=landmark_array[19]\n",
        "                to_coordinate=landmark_array[24]\n",
        "                cutout_guide.append(from_coordinate)\n",
        "\n",
        "                for i in range (24, 26):\n",
        "                    from_coordinate=landmark_array[i]\n",
        "                    to_coordinate=landmark_array[i+1]\n",
        "                    cutout_guide.append(from_coordinate)\n",
        "                \n",
        "                from_coordinate=landmark_array[26]\n",
        "                to_coordinate=landmark_array[16]\n",
        "                cutout_guide.append(from_coordinate)\n",
        "                cutout_guide.append(to_coordinate)\n",
        "\n",
        "#outline face===============================================================\n",
        "                for i in range(0, len(cutout_guide)-1):\n",
        "                    from_coordinates=cutout_guide[i]\n",
        "                    to_coordinates=cutout_guide[i+1]\n",
        "                    IMG2=cv2.line(IMG, from_coordinate, to_coordinate, (255,255,255), 1)\n",
        "            \n",
        "#crop it    ================================================================\n",
        "                mask=np.zeros((IMG2.shape[0], IMG2.shape[1]))\n",
        "                mask=cv2.fillConvexPoly(mask, np.array(cutout_guide), 1)\n",
        "                mask=mask.astype(np.bool_)\n",
        "                cropped=np.zeros_like(IMG2)\n",
        "                cropped[mask]=IMG2[mask]\n",
        "\n",
        "#convert to rgb to avoid blue image from BGR ===============================\n",
        "                IMG=cv2.cvtColor(IMG, cv2.COLOR_BGR2RGB)\n",
        "                cropped=cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)\n",
        "                \n",
        "#jpg cropped  ==============================================================\n",
        "                IMG=cv2.resize(IMG, dim, interpolation = cv2.INTER_AREA)\n",
        "                cropped=cv2.resize(cropped, dim, interpolation = cv2.INTER_AREA)\n",
        "                data_division=int(frameCOUNT*0.9)\n",
        "                if f <= data_division:\n",
        "                    cv2.imwrite(A_train+'A{}.jpg'.format(f), IMG)\n",
        "                    cv2.imwrite(B_train+'B{}.jpg'.format(f), cropped)\n",
        "                elif f >= data_division and f <= (int(data_division*1.07)):\n",
        "                    cv2.imwrite(A_val+'A{}.jpg'.format(f), IMG)\n",
        "                    cv2.imwrite(B_val+'B{}.jpg'.format(f), cropped)\n",
        "                else:\n",
        "                    cv2.imwrite(A_test+'A{}.jpg'.format(f), IMG)\n",
        "                    cv2.imwrite(B_test+'B{}.jpg'.format(f), cropped)\n",
        "                f+=1\n",
        "            except Exception:\n",
        "                pass\n",
        "    return croppedFACES, A_folder, B_folder\n",
        "\n",
        "\n",
        "croppedFACES, A_folder, B_folder=FACE_CROPPER(RealESRGAN_result_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6KmmiF1vswun"
      },
      "outputs": [],
      "source": [
        "#@title load pix2pix \n",
        "#@markdown optionally download git if its not already in work_folder:\n",
        "download_pix2pix = True #@param {type:\"boolean\"}\n",
        "if download_pix2pix==True:\n",
        "    !git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix {pix2pix_folder}\n",
        "%cd {pix2pix_folder}\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N8WuUV0kLV3w"
      },
      "outputs": [],
      "source": [
        "#@title Format DATA for train pix2pix GAN.\n",
        "#@markdown (A, B, and AB with corresponding \"test,\" \"train\" and \"val\" folder.)\n",
        "#croppedFACES=RESULTS+'{}_crop/'.format(search_image)\n",
        "\n",
        "#A_folder=croppedFACES+'A/'\n",
        "#B_folder=croppedFACES+'B/'\n",
        "AB_folder=croppedFACES+'AB/'\n",
        "foldermaker(AB_folder)\n",
        "\n",
        "def combine_AB(A, B, path_AB):\n",
        "    Aa, frameCOUNT=list_loader(A)\n",
        "    Bb, frameCOUNT=list_loader(B)\n",
        "    foldermaker(path_AB)\n",
        "    print(frameCOUNT)\n",
        "\n",
        "    for f in range (0, frameCOUNT):\n",
        "        print(f)\n",
        "        path_A=A+Aa[f]\n",
        "        path_B=B+Bb[f]\n",
        "    \n",
        "        im_A = cv2.imread(path_A) # python2: cv2.CV_LOAD_IMAGE_COLOR; python3: cv2.IMREAD_COLOR\n",
        "        im_B = cv2.imread(path_B) # python2: cv2.CV_LOAD_IMAGE_COLOR; python3: cv2.IMREAD_COLOR\n",
        "        im_AB = np.concatenate([im_A, im_B],1)\n",
        "\n",
        "        cv2.imwrite(path_AB+'{}_AB{}.jpg'.format(search_image,f), im_AB)\n",
        "        f+=1\n",
        "\n",
        "combine_AB(A_folder+'train/', B_folder+'train/', AB_folder+'train/')\n",
        "combine_AB(A_folder+'test/', B_folder+'test/', AB_folder+'test/')\n",
        "combine_AB(A_folder+'val/', B_folder+'val/', AB_folder+'val/')\n",
        "\n",
        "#!python datasets/combine_A_and_B.py --fold_A {A_folder} --fold_B {B_folder} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh_3QdJzqz_d"
      },
      "outputs": [],
      "source": [
        "#@title Train pix2pix model with formatted Data. { display-mode: \"form\" }\n",
        "#@markdown Name checkpoint to reuse when generating images.\n",
        "%cd {pix2pix_folder}\n",
        "model_checkpoint = \"mugshotGAN_4\" #@param {type:\"string\"}\n",
        "croppedFACES_ab=RESULTS+search_image+'_crop/'+'AB/'\n",
        "#@markdown Select direction of generation in GAN and total epochs:\n",
        "model_direction = \"BtoA\" #@param [\"AtoB\", \"BtoA\"]\n",
        "num_epochs = \"4000\" #@param {type:\"string\"}\n",
        "#@markdown To continue training check:\n",
        "continue_training = True #@param {type:\"boolean\"}\n",
        "if model_checkpoint == \"mugshotGAN_4\" and continue_training ==True:\n",
        "    %cd {pix2pix_folder+'checkpoints/'}\n",
        "    !wget -nc https://mint.tokyo/MUGSHOT_GAN/mugshotGAN_4.zip\n",
        "    !sudo unzip mugshotGAN_4.zip\n",
        "    %cd \n",
        "    %cd {pix2pix_folder}\n",
        "if continue_training == True:\n",
        "    restart_epoch = \"2150\" #@param {type:\"string\"}\n",
        "    continue_params='--continue_train --epoch_count {}'.format(restart_epoch)\n",
        "else:\n",
        "    continue_params=''\n",
        "#--epoch_count 333 --continue_train --n_epochs 900 --n_epochs_decay 900 --ndf 124 --ngf 124 --load_size 572 --crop_size 512\n",
        "!python train.py --dataroot {croppedFACES_ab} --name {model_checkpoint} --model pix2pix --direction {model_direction} --n_epochs {num_epochs} {continue_params} "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSk3KXlIYhMs"
      },
      "source": [
        "\n",
        "# Get NU IMAGES to Mugshot-ify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-a7xlOUC0ngN"
      },
      "outputs": [],
      "source": [
        "#@title Set target face variable.\n",
        "#@markdown \n",
        "#@markdown\n",
        "#@markdown Whose face do you want to download:\n",
        "#@markdown \n",
        "search_image_nu = 'capi' #@param {type: 'string'}\n",
        "search_image_nu=search_image_nu.replace(' ', '_') \n",
        "image_folder_nu=RESULTS+search_image_nu+'/'\n",
        "\n",
        "torch.save(search_image_nu, f'{mugshotGAN_folder}search_image_nu.torch', _use_new_zipfile_serialization=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtRKw4t72wgU"
      },
      "outputs": [],
      "source": [
        "#@title Choose DOWNLOAD or UPLOAD images. { form-width: \"200px\", display-mode: \"form\" }\n",
        "face_type = \"upload\" #@param [\"download\", \"upload\"] {allow-input: true}\n",
        "how_many_dl_imgs = \"20000\" #@param [\"100\", \"200\", \"500\", \"2000\"] {allow-input: true}\n",
        "#@markdown If you chose upload, use one of the following or LEAVE BLANK to UPLOAD from browser.\n",
        "\n",
        "video_boi = True #@param {type:\"boolean\"}\n",
        "zip_boi = False #@param {type:\"boolean\"}\n",
        "path_boi = \"/content/drive/MyDrive/mugshotGAN/RESULTS/capi_VIDS/\" #@param {type:\"string\"}\n",
        "\n",
        "if face_type == 'download':\n",
        "    %cd {mugshotGAN_folder}\n",
        "    !googleimagesdownload -k {search_image_nu} --chromedriver /usr/bin/chromedriver -ri -l {how_many_dl_imgs} -o {RESULTS} -s large\n",
        "elif face_type == 'upload' and video_boi == False and zip_boi ==False and path_boi==\"\":\n",
        "    %cd \n",
        "    %cd {work_folder}\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "        print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "            name=fn, length=len(uploaded[fn])))\n",
        "    image_folder_nu='/content/drive/MyDrive/'\n",
        "\n",
        "elif face_type == 'upload' and video_boi == False and zip_boi ==False:\n",
        "    %cd\n",
        "    %cd {work_folder}\n",
        "    image_folder_nu=path_boi\n",
        "\n",
        "elif face_type == 'upload' and video_boi == True and path_boi!='':\n",
        "    %cd\n",
        "    %cd {RESULTS+'pathboi_CAPI'}\n",
        "    def vid2frames(folder):\n",
        "        foldermaker(image_folder_nu)\n",
        "        vid_name, total_videos=list_loader(folder) \n",
        "    #load video ===================================================\n",
        "        for f in range(0,total_videos):\n",
        "            video=folder+vid_name[f]\n",
        "            vidcap=cv2.VideoCapture(f'{video}')\n",
        "            success, video=vidcap.read()\n",
        "            cv2_imshow(video)\n",
        "            cv2.waitKey(1)\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "#get number of frames in vid =================================\n",
        "            #total_frames=count_frames(video)\n",
        "            count=0\n",
        "            #print(total_frames)\n",
        "            print(len(video))\n",
        "#vertical 2 horizontal vid====================================\n",
        "            vid_rot=input('rotate?(y,n):')\n",
        "            while success:\n",
        "\n",
        "#rotate first ===============================================\n",
        "                if vid_rot == 'y':\n",
        "                    ROT_angle={'90': cv2.ROTATE_90_CLOCKWISE,\n",
        "                    '-90': cv2.ROTATE_90_COUNTERCLOCKWISE, \n",
        "                    '180': cv2.ROTATE_180}\n",
        "                    angle=input('rotate angle?:90,-90,180:')\n",
        "                    rota=ROT_angle.get(angle)\n",
        "                    video=cv2.rotate(video, rota)\n",
        "#save as jpg ================================================\n",
        "                    cv2.imwrite(image_folder_nu+'{}_vid{}_f{}.jpg'.format(search_image_nu, f, count), video)\n",
        "                    success,video=vidcap.read()\n",
        "                    print('Read a new frame: ', success)\n",
        "                    count+=1\n",
        "                    f += 1\n",
        "                    print(\"EUREEEEEEEKA\")\n",
        "                else:\n",
        "                    cv2.imwrite(image_folder_nu+'{}_vid{}_f{}.jpg'.format(search_image_nu, f, count), video)\n",
        "                    success,video=vidcap.read()\n",
        "                    print('Read a new frame: ', success)\n",
        "                    count+=1\n",
        "                    f += 1\n",
        "                    print(\"EUREEEEEEEKA\")\n",
        "#vid function =======================================================    \n",
        "    vid2frames(path_boi)\n",
        "else:\n",
        "    from google.colab import files\n",
        "#video boi=====================================================\n",
        "    if video_boi == True:\n",
        "        %cd\n",
        "        %cd {work_folder}\n",
        "        vid_folder=RESULTS+'{}_VIDS/'.format(search_image_nu)\n",
        "        foldermaker(vid_folder)\n",
        "        image_folder_nu=vid_folder\n",
        "        %cd {vid_folder}\n",
        "        vid_name=[]\n",
        "        uploaded = files.upload()\n",
        "        for fn in uploaded.keys():\n",
        "            print('User uploaded file \"{name}\" with length {length} bytes'.format(        name=fn, length=len(uploaded[fn])))\n",
        "            vid_name.append(fn)\n",
        "        from google.colab.patches import cv2_imshow\n",
        "        def vid2frames(uploaded):\n",
        "    #load vid path=================================================\n",
        "            if uploaded== None:\n",
        "                uploaded=input('video folder:')\n",
        "            foldermaker(image_folder_nu)\n",
        "            total_videos=len(uploaded) \n",
        "    #load video ===================================================\n",
        "            for f in range(0,total_videos):\n",
        "                video=vid_folder+vid_name[f]\n",
        "                vidcap=cv2.VideoCapture(f'{video}')\n",
        "                success, video=vidcap.read()\n",
        "                cv2_imshow(video)\n",
        "                cv2.waitKey(0)\n",
        "                \n",
        "                input \n",
        "                cv2.destroyAllWindows()\n",
        "                \n",
        "    #get number of frames in vid =================================\n",
        "                #total_frames=count_frames(video)\n",
        "                count=0\n",
        "                #print(total_frames)\n",
        "                #print(len(video))\n",
        "    #vertical 2 horizontal vid====================================\n",
        "                while success:\n",
        "\n",
        "    #rotate first ===============================================\n",
        "                    if vid_rot == 'y':\n",
        "                        ROT_angle={'90': cv2.ROTATE_90_CLOCKWISE,\n",
        "                        '-90': cv2.ROTATE_90_COUNTERCLOCKWISE, \n",
        "                        '180': cv2.ROTATE_180}\n",
        "                        angle=input('rotate angle?:90,-90,180:')\n",
        "                        rota=ROT_angle.get(angle)\n",
        "                        video=cv2.rotate(video, rota)\n",
        "    #save as jpg ================================================\n",
        "                        cv2.imwrite(image_folder_nu+'{}_vid{}_f{}.jpg'.format(search_image_nu, f, count), video)\n",
        "                        success,video=vidcap.read()\n",
        "                        print('Read a new frame: ', success)\n",
        "                        count+=1\n",
        "                        f += 1\n",
        "                        print(\"EUREEEEEEEKA\")\n",
        "                    else:\n",
        "                        cv2.imwrite(image_folder_nu+'{}_vid{}_f{}.jpg'.format(search_image_nu, f, count), video)\n",
        "                        success,video=vidcap.read()\n",
        "                        print('Read a new frame: ', success)\n",
        "                        count+=1\n",
        "                        f += 1\n",
        "                        print(\"EUREEEEEEEKA\")\n",
        "#vid function =======================================================    \n",
        "        vid2frames(uploaded)\n",
        "#zip boi=============================================================\n",
        "    if zip_boi == True:\n",
        "        %cd\n",
        "        %cd {RESULTS}\n",
        "        uploaded = files.upload()\n",
        "        zip_name=[]\n",
        "        for fn in uploaded.keys():\n",
        "            print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "                name=fn, length=len(uploaded[fn])))\n",
        "            zip_name.append(fn)\n",
        "        import zipfile\n",
        "        import shutil\n",
        "        local_path = image_folder_nu\n",
        "        makefolder(local_path)\n",
        "        for f in  range (0, len(uploaded)):\n",
        "            file_name = zip_name[f]#zip_folder #path + dataset\n",
        "            with zipfile.ZipFile(file_name, 'r') as zip:\n",
        "                #zip.printdir()\n",
        "                print('Extracting all the files now...') \n",
        "                zip.extractall(local_path) \n",
        "                print('Done!')\n",
        "            f+=1\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndYz4aoKcSYX"
      },
      "source": [
        "# Test pix2pix on NU IMAGES."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_tGmy4Hu5MvN"
      },
      "outputs": [],
      "source": [
        "#@title Download and/or load JoJoGAN models. { form-width: \"100px\" }\n",
        "#@markdown RUN THIS STEP and download with pydrive during FIST RUN. You may skip pydrive download if models already in folder.\n",
        "#\n",
        "#@markdown Defaults to downloaded models. SMASH THAT GO BUTTON!\n",
        "%cd {JoJo_folder}\n",
        "\n",
        "download_with_pydrive = False #@param {type:\"boolean\"}    \n",
        "device = 'cuda' #@param ['cuda', 'cpu']\n",
        "\n",
        "!wget -nc http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "!mv shape_predictor_68_face_landmarks.dat models/dlibshape_predictor_68_face_landmarks.dat\n",
        "#!cp {JoJoMODELS}shape_predictor_68_face_landmarks.dat models/dlibshape_predictor_68_face_landmarks.dat\n",
        "#!cp {JoJoMODELS}stylegan2-ffhq-config-f.pt models/stylegan2-ffhq-config-f.pt\n",
        "#!cp {JoJoMODELS}e4e_ffhq_encode.pt models/e4e_ffhq_encode.pt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "drive_ids = {\n",
        "    \"stylegan2-ffhq-config-f.pt\": \"1Yr7KuD959btpmcKGAUsbAk5rPjX2MytK\",\n",
        "    \"e4e_ffhq_encode.pt\": \"1o6ijA3PkcewZvwJJ73dJ0fxhndn0nnh7\",\n",
        "    \"restyle_psp_ffhq_encode.pt\": \"1nbxCIVw9H3YnQsoIPykNEFwWJnHVHlVd\",\n",
        "    \"arcane_caitlyn.pt\": \"1gOsDTiTPcENiFOrhmkkxJcTURykW1dRc\",\n",
        "    \"arcane_caitlyn_preserve_color.pt\": \"1cUTyjU-q98P75a8THCaO545RTwpVV-aH\",\n",
        "    \"arcane_jinx_preserve_color.pt\": \"1jElwHxaYPod5Itdy18izJk49K1nl4ney\",\n",
        "    \"arcane_jinx.pt\": \"1quQ8vPjYpUiXM4k1_KIwP4EccOefPpG_\",\n",
        "    \"arcane_multi_preserve_color.pt\": \"1enJgrC08NpWpx2XGBmLt1laimjpGCyfl\",\n",
        "    \"arcane_multi.pt\": \"15V9s09sgaw-zhKp116VHigf5FowAy43f\",\n",
        "    \"sketch_multi.pt\": \"1GdaeHGBGjBAFsWipTL0y-ssUiAqk8AxD\",\n",
        "    \"disney.pt\": \"1zbE2upakFUAx8ximYnLofFwfT8MilqJA\",\n",
        "    \"disney_preserve_color.pt\": \"1Bnh02DjfvN_Wm8c4JdOiNV4q9J7Z_tsi\",\n",
        "    \"jojo.pt\": \"13cR2xjIBj8Ga5jMO7gtxzIJj2PDsBYK4\",\n",
        "    \"jojo_preserve_color.pt\": \"1ZRwYLRytCEKi__eT2Zxv1IlV6BGVQ_K2\",\n",
        "    \"jojo_yasuho.pt\": \"1grZT3Gz1DLzFoJchAmoj3LoM9ew9ROX_\",\n",
        "    \"jojo_yasuho_preserve_color.pt\": \"1SKBu1h0iRNyeKBnya_3BBmLr4pkPeg_L\",\n",
        "    \"art.pt\": \"1a0QDEHwXQ6hE_FcYEyNMuv5r5UnRQLKT\",\n",
        "}\n",
        "\n",
        "# from StyelGAN-NADA\n",
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "        \n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "    \n",
        "    def download_file(self, file_name):\n",
        "        file_dst = os.path.join('models', file_name)\n",
        "        file_id = drive_ids[file_name]\n",
        "        if not os.path.exists(file_dst):\n",
        "            print(f'Downloading {file_name}')\n",
        "            if self.use_pydrive:\n",
        "                downloaded = self.drive.CreateFile({'id':file_id})\n",
        "                downloaded.FetchMetadata(fetch_all=True)\n",
        "                downloaded.GetContentFile(file_dst)\n",
        "            else:\n",
        "                !gdown --id $file_id -O $file_dst\n",
        "\n",
        "\n",
        "if download_with_pydrive == True:\n",
        "    downloader = Downloader(download_with_pydrive)\n",
        "    downloader.download_file('stylegan2-ffhq-config-f.pt')\n",
        "    downloader.download_file('e4e_ffhq_encode.pt')\n",
        "\n",
        "latent_dim = 512\n",
        "\n",
        "# Load original generator\n",
        "original_generator = Generator(1024, latent_dim, 8, 2).to(device)\n",
        "ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
        "original_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
        "mean_latent = original_generator.mean_latent(10000)\n",
        "\n",
        "# to be finetuned generator\n",
        "generator = deepcopy(original_generator)\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((1024, 1024)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cCe_zMw26MVX"
      },
      "outputs": [],
      "source": [
        "#@title Run JoJo face detect, resize and crop TARGET faces. { form-width: \"200px\" }\n",
        "\n",
        "##@markdown Choose OUTPUT type:\n",
        "#jojo_facedetect_output = \"jpgs_and_variables\" #@param [\"jpgs_and_variables\", \"only_variables\"]\n",
        "#@markdown Choose processing device type:\n",
        "device_type = \"cuda\" #@param [\"cuda\", \"cpu\"]\n",
        "def jojo_cuda_DetectResize(folder=image_folder_nu, search_image=search_image_nu):\n",
        "\n",
        "#cuda as device ===================================================================\n",
        "    device = device_type\n",
        "\n",
        " #result folder n variable ========================================================   \n",
        "    FACES=RESULTS+search_image+'_FACES/'\n",
        "    foldermaker(FACES)\n",
        "    aligned_faces=[]\n",
        "    my_ws=[]\n",
        "\n",
        "#load folder======================================================================\n",
        "    filepath, total_files=list_loader(folder)\n",
        "\n",
        "#loop thru folder=================================================================\n",
        "    for f in range (0,total_files):\n",
        "        img=folder+filepath[f]\n",
        "        #print(img)\n",
        "\n",
        "#skip folders n weird formats ===================================================\n",
        "        img_extensions=('.jpg', '.png')\n",
        "        if img.endswith(img_extensions):\n",
        "            try:\n",
        "                plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# filepath = list(uploaded.keys())[0]\n",
        "                name = strip_path_extension(img)+'.pt'\n",
        "\n",
        "# aligns and crops face =====================\n",
        "                aligned_face= align_face(img)\n",
        "                aligned_face.save(FACES+'{}_JOJOface_{}.jpg'.format(search_image, f), \"JPEG\", quality=100, optimize=True, progressive=True)\n",
        "                my_w = e4e_projection(aligned_face, name, device).unsqueeze(0)\n",
        "                #print(aligned_face)\n",
        "                #display_image(aligned_face, title='Aligned face')\n",
        "                aligned_faces.append(aligned_face)\n",
        "                my_ws.append(my_w)\n",
        "                f+=1\n",
        "            except Exception:\n",
        "                pass\n",
        "    return (FACES, device, aligned_faces, my_ws)\n",
        "\n",
        "\n",
        "def jojo_cuda_DetectResize_nosave(folder=image_folder_nu):\n",
        "\n",
        "#cuda as device ===================================================================\n",
        "    device = device_type\n",
        "\n",
        " #result folder n variable ========================================================   \n",
        "    FACES=RESULTS+search_image+'_FACES/'\n",
        "    foldermaker(FACES)\n",
        "    aligned_faces=[]\n",
        "    my_ws=[]\n",
        "\n",
        "#load folder======================================================================\n",
        "    filepath, total_files=list_loader(folder)\n",
        "\n",
        "#loop thru folder=================================================================\n",
        "    for f in range (0,total_files):\n",
        "        img=folder+filepath[f]\n",
        "        #print(img)\n",
        "\n",
        "#skip folders n weird formats ===================================================\n",
        "        img_extensions=('.jpg', '.png')\n",
        "        if img.endswith(img_extensions):\n",
        "            try:\n",
        "                plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# filepath = list(uploaded.keys())[0]\n",
        "                name = strip_path_extension(img)+'.pt'\n",
        "\n",
        "# aligns and crops face =====================\n",
        "                aligned_face= align_face(img)\n",
        "                #aligned_face.save(FACES+'{}_JOJOface_{}.jpg'.format(search_image, f), \"JPEG\", quality=100, optimize=True, progressive=True)\n",
        "\n",
        "                # my_w = restyle_projection(aligned_face, name, device, n_iters=1).unsqueeze(0)\n",
        "                my_w = e4e_projection(aligned_face, name, device).unsqueeze(0)\n",
        "\n",
        "                #print(aligned_face)\n",
        "                #display_image(aligned_face, title='Aligned face')\n",
        "                my_ws.append(my_w)\n",
        "                aligned_faces.append(aligned_face)\n",
        "                f+=1\n",
        "            except Exception:\n",
        "                pass\n",
        "    return (FACES, device, aligned_faces, my_ws)\n",
        "\n",
        "#@markdown *This cell autosaves a pytorch checkpoint VARS to resume in case of crash.\n",
        "\n",
        "FACES, device, aligned_faces, my_ws=jojo_cuda_DetectResize(image_folder_nu, search_image_nu)\n",
        "torch.save(aligned_faces, f'{mugshotGAN_folder}{search_image_nu}_aligned_faces.torch', _use_new_zipfile_serialization=False)\n",
        "torch.save(my_ws, f'{JoJo_folder}{search_image_nu}_my_ws.torch', _use_new_zipfile_serialization=False)\n",
        "#if jojo_facedetect_output == \"jpgs_and_variables\":\n",
        " #   FACES, device, aligned_faces, my_ws=jojo_cuda_DetectResize(image_folder_nu, search_image_nu)\n",
        "  #  torch.save(aligned_faces, f'{mugshotGAN_folder}{search_image_nu}_aligned_faces.torch', _use_new_zipfile_serialization=False)\n",
        "   # torch.save(my_ws, f'{JoJo_folder}{search_image_nu}_my_ws.torch', _use_new_zipfile_serialization=False)\n",
        "#else:\n",
        " #   print(image_folder_nu)\n",
        "  #  FACES, device, aligned_faces, my_ws=jojo_cuda_DetectResize_nosave(image_folder_nu)\n",
        "   # torch.save(aligned_faces, f'{mugshotGAN_folder}{search_image_nu}_aligned_faces.torch', _use_new_zipfile_serialization=False)\n",
        "    #torch.save(my_ws, f'{JoJo_folder}{search_image_nu}_my_ws.torch', _use_new_zipfile_serialization=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxWjb55FzDdU"
      },
      "outputs": [],
      "source": [
        "#@title Crop new faces: { form-width: \"120px\", display-mode: \"form\" }\n",
        "#RealESRGAN_result_folder_nu=RESULTS+search_image_nu+'_ESRGAN/'\n",
        "RealESRGAN_result_folder_nu=RESULTS+'{}_FACES/'.format(search_image_nu)\n",
        "\n",
        "#@markdown Size should match training size:\n",
        "width_and_height = 1024 #@param [\"256\", \"512\", \"1024\"] {type:\"raw\"}\n",
        "\n",
        "import cv2\n",
        "import dlib\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def FACE_CROPPER_nuface (folder, width=width_and_height, height=width_and_height):\n",
        "    croppedFACES_nu=RESULTS+'{}_crop/'.format(search_image_nu)\n",
        "    foldermaker(croppedFACES_nu)\n",
        "    \n",
        "    A_folder_nu=croppedFACES_nu+'A/'\n",
        "    B_folder_nu=croppedFACES_nu+'B/'\n",
        "    foldermaker(A_folder_nu)\n",
        "    foldermaker(B_folder_nu)\n",
        "\n",
        "    A_train_nu=A_folder_nu+'train/'\n",
        "    B_train_nu=B_folder_nu+'train/'\n",
        "    foldermaker(A_train_nu)\n",
        "    foldermaker(B_train_nu)\n",
        "\n",
        "    A_val_nu=A_folder_nu+'val/'\n",
        "    B_val_nu=B_folder_nu+'val/'\n",
        "    foldermaker(A_val_nu)\n",
        "    foldermaker(B_val_nu)\n",
        "    \n",
        "    A_test_nu=A_folder_nu+'test/'\n",
        "    B_test_nu=B_folder_nu+'test/'\n",
        "    foldermaker(A_test_nu)\n",
        "    foldermaker(B_test_nu)\n",
        "    \n",
        "    dim=(width, height)\n",
        "\n",
        "#model ================================================================\n",
        "    detector=dlib.get_frontal_face_detector()\n",
        "    model=dlib.shape_predictor(JoJo_folder+'models/dlibshape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "#source=================================================================\n",
        "    FACES, frameCOUNT=list_loader(folder)\n",
        "\n",
        "#loop trhu folder ======================================================\n",
        "    for f in range(0,frameCOUNT):\n",
        "        print(f)\n",
        "\n",
        "#string path ===========================================================\n",
        "        imash=folder+FACES[f]\n",
        "        img_extensions=('.jpg','.png')\n",
        "\n",
        "#only JPGS AND PNGS  ===================================================\n",
        "        if imash.endswith(img_extensions):\n",
        "            try:\n",
        "\n",
        "#DLIB LOAD==============================================================\n",
        "                IMG=dlib.load_rgb_image(imash)   \n",
        "#CV LOAD  ==============================================================\n",
        "                #IMG = cv2.imread(imash)[:,:,::-1]\n",
        "            \n",
        "#find face =============================================================\n",
        "                FACE=detector(IMG, 1)\n",
        "            \n",
        "#Pass2landmark =========================================================\n",
        "                landmark_array=[]\n",
        "                for k, d, in enumerate(FACE):\n",
        "                    landmarks=model(IMG, d)\n",
        "\n",
        "#select only landmarks 0-27 to cutout face=============================\n",
        "                    for n in range(0,27):\n",
        "                        x=landmarks.part(n).x\n",
        "                        y=landmarks.part(n).y\n",
        "                        landmark_array.append((x, y))\n",
        "                        #cv2.circle(IMG, (x,y), 2, (255, 255, 0), -1)\n",
        "            \n",
        " #detect if face is frontal (\"manually\")\n",
        "                frontson1=landmark_array[0][1] \n",
        "                frontson2=landmark_array[17][1]\n",
        "\n",
        "                cutout_guide=[]\n",
        "\n",
        "#outline path from landmarks 0-27 ===================================\n",
        "                for i in range (15, -1, -1):\n",
        "                    from_coordinate=landmark_array[i+1]\n",
        "                    to_coordinate=landmark_array[i]\n",
        "                    cutout_guide.append(from_coordinate)\n",
        "            \n",
        "                from_coordinate=landmark_array[0]\n",
        "                to_coordinate=landmark_array[17]\n",
        "                cutout_guide.append(from_coordinate)\n",
        "\n",
        "                for i in range (17, 20):\n",
        "                    from_coordinate=landmark_array[i]\n",
        "                    to_coordinate=landmark_array[i+1]\n",
        "                    cutout_guide.append(from_coordinate)\n",
        "            \n",
        "                from_coordinate=landmark_array[19]\n",
        "                to_coordinate=landmark_array[24]\n",
        "                cutout_guide.append(from_coordinate)\n",
        "\n",
        "                for i in range (24, 26):\n",
        "                    from_coordinate=landmark_array[i]\n",
        "                    to_coordinate=landmark_array[i+1]\n",
        "                    cutout_guide.append(from_coordinate)\n",
        "                \n",
        "                from_coordinate=landmark_array[26]\n",
        "                to_coordinate=landmark_array[16]\n",
        "                cutout_guide.append(from_coordinate)\n",
        "                cutout_guide.append(to_coordinate)\n",
        "\n",
        "#outline face===============================================================\n",
        "                for i in range(0, len(cutout_guide)-1):\n",
        "                    from_coordinates=cutout_guide[i]\n",
        "                    to_coordinates=cutout_guide[i+1]\n",
        "                    IMG2=cv2.line(IMG, from_coordinate, to_coordinate, (255,255,255), 1)\n",
        "            \n",
        "#crop it    ================================================================\n",
        "                mask=np.zeros((IMG2.shape[0], IMG2.shape[1]))\n",
        "                mask=cv2.fillConvexPoly(mask, np.array(cutout_guide), 1)\n",
        "                mask=mask.astype(np.bool_)\n",
        "                cropped=np.zeros_like(IMG2)\n",
        "                cropped[mask]=IMG2[mask]\n",
        "\n",
        "#convert to rgb to avoid blue image from BGR ===============================\n",
        "                IMG=cv2.cvtColor(IMG, cv2.COLOR_BGR2RGB)\n",
        "                cropped=cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)\n",
        "                \n",
        "#jpg cropped  ==============================================================\n",
        "                IMG=cv2.resize(IMG, dim, interpolation = cv2.INTER_AREA)\n",
        "                cropped=cv2.resize(cropped, dim, interpolation = cv2.INTER_AREA)\n",
        "                data_division=int(frameCOUNT*0.92)\n",
        "                if f <= data_division:\n",
        "                    cv2.imwrite(B_test_nu+'{}_crop{}.jpg'.format(search_image_nu, f), cropped)\n",
        "                    cv2.imwrite(A_test_nu+'A{}.jpg'.format(f), IMG)\n",
        "                elif f >= data_division and f <= (int(data_division*1.15)):\n",
        "                    cv2.imwrite(A_val_nu+'A{}.jpg'.format(f), IMG)\n",
        "                    cv2.imwrite(B_val_nu+'B{}.jpg'.format(f), cropped)\n",
        "                else:\n",
        "                    cv2.imwrite(A_train_nu+'A{}.jpg'.format(f), IMG)\n",
        "                    cv2.imwrite(B_train_nu+'B{}.jpg'.format(f), cropped)\n",
        "                f+=1\n",
        "            except Exception:\n",
        "                pass\n",
        "    return croppedFACES_nu\n",
        "\n",
        "croppedFACES_nu=FACE_CROPPER_nuface(RealESRGAN_result_folder_nu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SuuS_zyesum0"
      },
      "outputs": [],
      "source": [
        "#@title Format DATA to test pix2pix GAN.\n",
        "#@markdown (A, B, and AB with corresponding \"test,\" \"train\" and \"val\" folder.)\n",
        "#croppedFACES=RESULTS+'{}_crop/'.format(search_image)\n",
        "\n",
        "A_folder_nu=croppedFACES_nu+'A/'\n",
        "B_folder_nu=croppedFACES_nu+'B/'\n",
        "AB_folder_nu=croppedFACES_nu+'AB/'\n",
        "foldermaker(AB_folder_nu)\n",
        "\n",
        "def combine_AB(A, B, path_AB):\n",
        "    Aa, frameCOUNT=list_loader(A)\n",
        "    Bb, frameCOUNT=list_loader(B)\n",
        "    foldermaker(path_AB)\n",
        "    print(frameCOUNT)\n",
        "\n",
        "    for f in range (0, frameCOUNT):\n",
        "        print(f)\n",
        "        path_A=A+Aa[f]\n",
        "        path_B=B+Bb[f]\n",
        "    \n",
        "        im_A = cv2.imread(path_A) # python2: cv2.CV_LOAD_IMAGE_COLOR; python3: cv2.IMREAD_COLOR\n",
        "        im_B = cv2.imread(path_B) # python2: cv2.CV_LOAD_IMAGE_COLOR; python3: cv2.IMREAD_COLOR\n",
        "        im_AB = np.concatenate([im_A, im_B],1)\n",
        "\n",
        "        cv2.imwrite(path_AB+'{}_AB{}.jpg'.format(search_image_nu,f), im_AB)\n",
        "        f+=1\n",
        "combine_AB(A_folder_nu+'train/', B_folder_nu+'train/', AB_folder_nu+'train/')\n",
        "combine_AB(A_folder_nu+'test/', B_folder_nu+'test/', AB_folder_nu+'test/')\n",
        "combine_AB(A_folder_nu+'val/', B_folder_nu+'val/', AB_folder_nu+'val/')\n",
        "\n",
        "#!python datasets/combine_A_and_B.py --fold_A {A_folder} --fold_B {B_folder} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FkvRv58baSrs"
      },
      "outputs": [],
      "source": [
        "#@title load pix2pix \n",
        "#@markdown optionally download git if its not already in work_folder:\n",
        "download_pix2pix = False #@param {type:\"boolean\"}\n",
        "if download_pix2pix==True:\n",
        "    !git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix {pix2pix_folder}\n",
        "%cd {pix2pix_folder}\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V5ZIei4CNWie"
      },
      "outputs": [],
      "source": [
        "#@title Test pix2pix on new cropped faces.\n",
        "#@markdown (Setting VARS again for autonomy, also VERY helpful if crash.)\n",
        "\n",
        "#@markdown Use default mugshotGAN_4 checkpoint or use your version, just keep checkpoint name same as training. \n",
        "\n",
        "#@markdown (Make sure your checkpoint version is in pix2pix 'checkpoints' folder, if choosing mugshotGAN_4 it will DOWNLOAD from mint.tokyo & be there automatically.)\n",
        "model_checkpoint = \"mugshotGAN_4\" #@param [\"mugshotGAN_4\"] {allow-input: true}\n",
        "if model_checkpoint == \"mugshotGAN_4\":\n",
        "    %cd {pix2pix_folder+'checkpoints/'}\n",
        "    foldermaker(pix2pix_folder+'checkpoints/mugshotGAN_4/')\n",
        "    !wget -nc https://mint.tokyo/MUGSHOTGAN/MUGSHOTGAN_4.zip\n",
        "    !sudo unzip -n MUGSHOTGAN_4.zip -d {pix2pix_folder+'checkpoints/mugshotGAN_4'}\n",
        "    %cd\n",
        "    %cd {pix2pix_folder}\n",
        "#@markdown Keep same GAN direction too:\n",
        "model_direction = \"BtoA\" #@param [\"AtoB\", \"BtoA\"]\n",
        "\n",
        "#@markdown (if you are using the mugshotGAN_4 checkpoint, direction is BtoA)\n",
        "croppedFACES_nu=RESULTS+'{}_crop/'.format(search_image_nu)\n",
        "PIX2PIX_testdata, pix2pixtest_length=list_loader(croppedFACES_nu+'AB/test')\n",
        "print(croppedFACES_nu)\n",
        "pix2pix_results=RESULTS+'{}_PIX2PIX/'.format(search_image_nu)\n",
        "!python test.py --dataroot {croppedFACES_nu+'AB/'} --name {model_checkpoint} --model pix2pix --direction {model_direction} --num_test {pix2pixtest_length*4} --num_threads 8 --no_dropout --serial_batches --results_dir {pix2pix_results}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZdYrfVcJdqa"
      },
      "source": [
        "# Optionally, create JoJoGAN styles with your own images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZdSBaQ_A7BIH"
      },
      "outputs": [],
      "source": [
        "#@title Upload your own portraits for CUSTOM STYLE or skip these 2 steps for default MUGSHOT.\n",
        "#@markdown\n",
        "\n",
        "#@markdown ENABLE COOKIES 2 UPLOAD FROM BROWSER!\n",
        "\n",
        "#@markdown (you may manually place images directly in drive folder, just run this cell empty to create folder and format images with cell below)\n",
        "%cd\n",
        "JoJoSTYLES=mugshotGAN_folder+'jojo_styles/'\n",
        "foldermaker(JoJoSTYLES) \n",
        "upload_styles = False #@param {type:\"boolean\"}\n",
        "if upload_styles == True:\n",
        "    %cd {JoJoSTYLES}\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "        print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "            name=fn, length=len(uploaded[fn])))\n",
        "#copy_style_boi = False #@param {type:\"boolean\"}\n",
        "#if copy_style_boi==True:    \n",
        " #   !rsync -a {JoJoSTYLES} {JoJo_folder+'style_images'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyMWcEY98W5R"
      },
      "outputs": [],
      "source": [
        "#@title Convert STYLE imgs to .jpg & send to JoJo style_images folder: { form-width: \"200px\", display-mode: \"form\" }\n",
        "style_images_origin = JoJoSTYLES #@param [\"JoJoSTYLES\", \"image_folder\", \"image_folder_nu\"] {type:\"raw\", allow-input: true}\n",
        "def png_converter (folder, output):\n",
        "    from PIL import Image\n",
        "    IMG, image_count=list_loader(folder)\n",
        "    img_ext=('.jpg', '.png', '.PNG', '.JPG')\n",
        "    for f in range (0, image_count):\n",
        "        img=folder+IMG[f]\n",
        "        if img.endswith(img_ext):\n",
        "            #jpg_boi=Image.open(img)\n",
        "            jpg_superior=cv2.imread(img)\n",
        "            #rgb_boi=jpg_boi.convert('RGBX')\n",
        "            nu_img_name=strip_path_extension(IMG[f])\n",
        "            #rgb_boi.save(output+'{}.jpg'.format(nu_img_name), quality=110, color=32)\n",
        "            cv2.imwrite(output+'{}.jpg'.format(nu_img_name), jpg_superior, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
        "            print(nu_img_name+'.jpg saved in JoJo/style_images/')\n",
        "            f+=1\n",
        "png_converter(style_images_origin, JoJo_folder+'style_images/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7kqltYpM0JO"
      },
      "source": [
        "# generate final MUGSHOTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX54BXFAdFA_"
      },
      "outputs": [],
      "source": [
        "#@title Transfer PIX2PIX results into one MEGA folder. { display-mode: \"form\" }\n",
        "if search_image_nu != None:\n",
        "    mega_image_folder=RESULTS+'{}_mega_folder/'.format(search_image_nu)\n",
        "else:\n",
        "    mega_image_folder=RESULTS+'{}_mega_folder/'.format(search_image)\n",
        "foldermaker(mega_image_folder)\n",
        "#@markdown Enter folder path:\n",
        "pix2pix_results=RESULTS+'{}_PIX2PIX/'.format(search_image_nu)\n",
        "face_folder=pix2pix_results+'/mugshotGAN_4/test_latest/images/'#@param\n",
        "#!rsync -a {face_folder} {mega_image_folder}\n",
        "!rsync -am --include '*fake_B.png' --exclude '*real_A.png' --include '*real_B.png' {face_folder} {mega_image_folder} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8O01s_BG3zHJ"
      },
      "outputs": [],
      "source": [
        "#@title Run Real-ESRGAN to upscale the generated image DATASET.\n",
        "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git {RealESRGAN_folder}\n",
        "%cd {RealESRGAN_folder}\n",
        "# Set up the environment\n",
        "!pip install basicsr\n",
        "!pip install facexlib\n",
        "!pip install gfpgan\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "\n",
        "# Download the pre-trained model\n",
        "!wget -nc https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
        "\n",
        "import shutil\n",
        "JOJOfakes=RESULTS+'{}_FAKES/'.format(search_image_nu)\n",
        "os.makedirs('temp_inputs', exist_ok=True) \n",
        "FACES=RESULTS+'{}_FACES/'.format(search_image_nu)\n",
        "RealESRGAN_load_folder = FACES\n",
        "RealESERGAN_upload_folder=RealESRGAN_folder+'temp_inputs/'\n",
        "#!rsync -a {RealESRGAN_load_folder} {RealESRGAN_upload_folder}\n",
        "JOJO_results = False \n",
        "if search_image_nu != None and JOJO_results == True:\n",
        "    RealESRGAN_result_folder=RESULTS+search_image_nu+'_jojoFAKE_ESRGAN/'\n",
        "elif search_image_nu != None:\n",
        "    fake_folder=RESULTS+search_image_nu+'_FAKE/'\n",
        "    RealESRGAN_result_folder = RESULTS+search_image_nu+'_FAKE_ESRGAN/'    \n",
        "else:\n",
        "    RealESRGAN_result_folder = RESULTS+search_image+'_ESRGAN/'\n",
        "foldermaker(RealESRGAN_result_folder)\n",
        "realERSGAN_input = mega_image_folder #@param [\"mega_image_folder\", \"FACES\", \"image_folder\", \"None\", \"face_folder\", \"fake_folder\", \"JOJOfakes\"] {type:\"raw\"}\n",
        "!python inference_realesrgan.py -n RealESRGAN_x4plus --input {realERSGAN_input} --outscale 4 --half --face_enhance --output {RealESRGAN_result_folder}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "audEQcU9A2t-"
      },
      "outputs": [],
      "source": [
        "#@title Download and/or load JoJoGAN models.\n",
        "#@markdown SKIP PYDRIVE DOWNLOAD if models in folder.\n",
        "#\n",
        "#@markdown Defaults to downloaded models. SMASH THAT GO BUTTON!\n",
        "%cd {JoJo_folder}\n",
        "\n",
        "download_with_pydrive = True #@param {type:\"boolean\"}    \n",
        "device = 'cuda' #@param ['cuda', 'cpu']\n",
        "\n",
        "!wget -nc http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "!mv shape_predictor_68_face_landmarks.dat models/dlibshape_predictor_68_face_landmarks.dat\n",
        "#!cp {JoJoMODELS}shape_predictor_68_face_landmarks.dat models/dlibshape_predictor_68_face_landmarks.dat\n",
        "#!cp {JoJoMODELS}stylegan2-ffhq-config-f.pt models/stylegan2-ffhq-config-f.pt\n",
        "#!cp {JoJoMODELS}e4e_ffhq_encode.pt models/e4e_ffhq_encode.pt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "drive_ids = {\n",
        "    \"stylegan2-ffhq-config-f.pt\": \"1Yr7KuD959btpmcKGAUsbAk5rPjX2MytK\",\n",
        "    \"e4e_ffhq_encode.pt\": \"1o6ijA3PkcewZvwJJ73dJ0fxhndn0nnh7\",\n",
        "    \"restyle_psp_ffhq_encode.pt\": \"1nbxCIVw9H3YnQsoIPykNEFwWJnHVHlVd\",\n",
        "    \"arcane_caitlyn.pt\": \"1gOsDTiTPcENiFOrhmkkxJcTURykW1dRc\",\n",
        "    \"arcane_caitlyn_preserve_color.pt\": \"1cUTyjU-q98P75a8THCaO545RTwpVV-aH\",\n",
        "    \"arcane_jinx_preserve_color.pt\": \"1jElwHxaYPod5Itdy18izJk49K1nl4ney\",\n",
        "    \"arcane_jinx.pt\": \"1quQ8vPjYpUiXM4k1_KIwP4EccOefPpG_\",\n",
        "    \"arcane_multi_preserve_color.pt\": \"1enJgrC08NpWpx2XGBmLt1laimjpGCyfl\",\n",
        "    \"arcane_multi.pt\": \"15V9s09sgaw-zhKp116VHigf5FowAy43f\",\n",
        "    \"sketch_multi.pt\": \"1GdaeHGBGjBAFsWipTL0y-ssUiAqk8AxD\",\n",
        "    \"disney.pt\": \"1zbE2upakFUAx8ximYnLofFwfT8MilqJA\",\n",
        "    \"disney_preserve_color.pt\": \"1Bnh02DjfvN_Wm8c4JdOiNV4q9J7Z_tsi\",\n",
        "    \"jojo.pt\": \"13cR2xjIBj8Ga5jMO7gtxzIJj2PDsBYK4\",\n",
        "    \"jojo_preserve_color.pt\": \"1ZRwYLRytCEKi__eT2Zxv1IlV6BGVQ_K2\",\n",
        "    \"jojo_yasuho.pt\": \"1grZT3Gz1DLzFoJchAmoj3LoM9ew9ROX_\",\n",
        "    \"jojo_yasuho_preserve_color.pt\": \"1SKBu1h0iRNyeKBnya_3BBmLr4pkPeg_L\",\n",
        "    \"art.pt\": \"1a0QDEHwXQ6hE_FcYEyNMuv5r5UnRQLKT\",\n",
        "}\n",
        "\n",
        "# from StyelGAN-NADA\n",
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "        \n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "    \n",
        "    def download_file(self, file_name):\n",
        "        file_dst = os.path.join('models', file_name)\n",
        "        file_id = drive_ids[file_name]\n",
        "        if not os.path.exists(file_dst):\n",
        "            print(f'Downloading {file_name}')\n",
        "            if self.use_pydrive:\n",
        "                downloaded = self.drive.CreateFile({'id':file_id})\n",
        "                downloaded.FetchMetadata(fetch_all=True)\n",
        "                downloaded.GetContentFile(file_dst)\n",
        "            else:\n",
        "                !gdown --id $file_id -O $file_dst\n",
        "\n",
        "\n",
        "\n",
        "#downloader = Downloader(download_with_pydrive)\n",
        "\n",
        "#downloader.download_file('stylegan2-ffhq-config-f.pt')\n",
        "#downloader.download_file('e4e_ffhq_encode.pt')\n",
        "\n",
        "latent_dim = 512\n",
        "\n",
        "# Load original generator\n",
        "original_generator = Generator(1024, latent_dim, 8, 2).to(device)\n",
        "ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
        "original_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
        "mean_latent = original_generator.mean_latent(10000)\n",
        "\n",
        "# to be finetuned generator\n",
        "generator = deepcopy(original_generator)\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((1024, 1024)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dr6hNxmZTx8"
      },
      "outputs": [],
      "source": [
        "#@title Run JoJo face detect, resize and crop TARGET faces. { form-width: \"400px\", display-mode: \"form\" }\n",
        "#@markdown Choose OUTPUT type:\n",
        "jojo_facedetect_output = \"only_variables\" #@param [\"jpgs_and_variables\", \"only_variables\"]\n",
        "\n",
        "\n",
        "def jojo_cuda_DetectResize(folder=image_folder_nu, search_image=search_image_nu):\n",
        "\n",
        "#cuda as device ===================================================================\n",
        "    device = 'cuda'\n",
        "\n",
        " #result folder n variable ========================================================   \n",
        "    FAKES=RESULTS+search_image_nu+'_FAKES/'\n",
        "    foldermaker(FAKES)\n",
        "    aligned_faces=[]\n",
        "    my_ws=[]\n",
        "\n",
        "#load folder======================================================================\n",
        "    filepath, total_files=list_loader(folder)\n",
        "    total_files1=int(total_files/2)\n",
        "#loop thru folder=================================================================\n",
        "    for f in range (0, total_files1):\n",
        "        img=folder+filepath[f]\n",
        "        #print(img)\n",
        "\n",
        "#skip folders n weird formats ===================================================\n",
        "        img_extensions=('.jpg', '.png')\n",
        "        if img.endswith(img_extensions):\n",
        "            try:\n",
        "                plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# filepath = list(uploaded.keys())[0]\n",
        "                name = strip_path_extension(img)+'.pt'\n",
        "\n",
        "# aligns and crops face =====================\n",
        "                aligned_face= align_face(img)\n",
        "                aligned_face.save(FACES+'{}_JOJOface_{}.jpg'.format(search_image, f), \"JPEG\", quality=100, optimize=True, progressive=True)\n",
        "                my_w = e4e_projection(aligned_face, name, device).unsqueeze(0)\n",
        "                #print(aligned_face)\n",
        "                #display_image(aligned_face, title='Aligned face')\n",
        "                aligned_faces.append(aligned_face)\n",
        "                my_ws.append(my_w)\n",
        "                f+=1\n",
        "            except Exception:\n",
        "                pass\n",
        "    \n",
        "    for f in range (total_files1, total_files):\n",
        "        img=folder+filepath[f]\n",
        "        #print(img)\n",
        "\n",
        "#skip folders n weird formats ===================================================\n",
        "        img_extensions=('.jpg', '.png')\n",
        "        if img.endswith(img_extensions):\n",
        "            try:\n",
        "                plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# filepath = list(uploaded.keys())[0]\n",
        "                name = strip_path_extension(img)+'.pt'\n",
        "\n",
        "# aligns and crops face =====================\n",
        "                aligned_face= align_face(img)\n",
        "                aligned_face.save(FACES+'{}_JOJOface_{}.jpg'.format(search_image, f), \"JPEG\", quality=100, optimize=True, progressive=True)\n",
        "                my_w = e4e_projection(aligned_face, name, device).unsqueeze(0)\n",
        "                #print(aligned_face)\n",
        "                #display_image(aligned_face, title='Aligned face')\n",
        "                aligned_faces.append(aligned_face)\n",
        "                my_ws.append(my_w)\n",
        "                f+=1\n",
        "            except Exception:\n",
        "                pass\n",
        "    return (FAKES, device, aligned_faces, my_ws)\n",
        "\n",
        "\n",
        "def jojo_cuda_DetectResize_nosave(folder=image_folder_nu):\n",
        "\n",
        "#cuda as device ===================================================================\n",
        "    device = 'cuda'\n",
        "\n",
        " #result folder n variable ========================================================   \n",
        "    #FAKES=RESULTS+search_image_nu+'_FAKES/'\n",
        "    #foldermaker(FAKES)\n",
        "    aligned_faces=[]\n",
        "    my_ws=[]\n",
        "\n",
        "#load folder======================================================================\n",
        "    filepath, total_files=list_loader(folder)\n",
        "    total_files1=int(total_files/2)\n",
        "#loop thru folder=================================================================\n",
        "    for f in range (0, total_files1):\n",
        "        img=folder+filepath[f]\n",
        "        #print(img)\n",
        "\n",
        "#skip folders n weird formats ===================================================\n",
        "        img_extensions=('.jpg', '.png')\n",
        "        if img.endswith(img_extensions):\n",
        "            try:\n",
        "                plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# filepath = list(uploaded.keys())[0]\n",
        "                name = strip_path_extension(img)+'.pt'\n",
        "\n",
        "# aligns and crops face =====================\n",
        "                aligned_face= align_face(img)\n",
        "                #aligned_face.save(FACES+'{}_JOJOface_{}.jpg'.format(search_image, f), \"JPEG\", quality=100, optimize=True, progressive=True)\n",
        "\n",
        "                # my_w = restyle_projection(aligned_face, name, device, n_iters=1).unsqueeze(0)\n",
        "                my_w = e4e_projection(aligned_face, name, device).unsqueeze(0)\n",
        "\n",
        "                #print(aligned_face)\n",
        "                #display_image(aligned_face, title='Aligned face')\n",
        "                my_ws.append(my_w)\n",
        "                aligned_faces.append(aligned_face)\n",
        "                f+=1\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    for f in range (total_files1, total_files):\n",
        "        img=folder+filepath[f]\n",
        "        #print(img)\n",
        "\n",
        "    #skip folders n weird formats ===================================================\n",
        "        img_extensions=('.jpg', '.png')\n",
        "        if img.endswith(img_extensions):\n",
        "            try:\n",
        "                plt.rcParams['figure.dpi'] = 150\n",
        "\n",
        "    # uploaded = files.upload()\n",
        "    # filepath = list(uploaded.keys())[0]\n",
        "                name = strip_path_extension(img)+'.pt'\n",
        "\n",
        "    # aligns and crops face =====================\n",
        "                aligned_face= align_face(img)\n",
        "                #aligned_face.save(FACES+'{}_JOJOface_{}.jpg'.format(search_image, f), \"JPEG\", quality=100, optimize=True, progressive=True)\n",
        "\n",
        "                # my_w = restyle_projection(aligned_face, name, device, n_iters=1).unsqueeze(0)\n",
        "                my_w = e4e_projection(aligned_face, name, device).unsqueeze(0)\n",
        "\n",
        "                #print(aligned_face)\n",
        "                #display_image(aligned_face, title='Aligned face')\n",
        "                my_ws.append(my_w)\n",
        "                aligned_faces.append(aligned_face)\n",
        "                f+=1\n",
        "            except Exception:\n",
        "                pass\n",
        "    return (device, aligned_faces, my_ws)\n",
        "\n",
        "#@markdown *This cell autosaves a pytorch checkpoint VARS to reuse in case of crash.\n",
        "\n",
        "run_after_pix2pix = True #@param {type:\"boolean\"}\n",
        "#use_fake_folder= False #@param {type:\"boolean\"}\n",
        "mega_image_folder=RESULTS+search_image_nu+'_mega_folder/'\n",
        "fake_folder=RESULTS+search_image_nu+'_FAKE/'\n",
        "if run_after_pix2pix == True :\n",
        "    RealESRGAN_result_folder=RESULTS+search_image_nu+'_FAKE_ESRGAN/'\n",
        "    image_folder_nu=mega_image_folder\n",
        "    #RealESRGAN_result_folder\n",
        "fake_input_folder = RealESRGAN_result_folder #@param [\"RealESRGAN_result_folder\", \"image_folder_nu\", \"mega_image_folder\"] {type:\"raw\", allow-input: true}\n",
        "if jojo_facedetect_output == \"jpgs_and_variables\":\n",
        "    FAKES, device, aligned_faces, my_ws=jojo_cuda_DetectResize(fake_input_folder, search_image_nu)\n",
        "    torch.save(aligned_faces, f'{mugshotGAN_folder}{search_image_nu}_aligned_faces.torch', _use_new_zipfile_serialization=False)\n",
        "    torch.save(my_ws, f'{JoJo_folder}{search_image_nu}_my_ws.torch', _use_new_zipfile_serialization=False)\n",
        "else:\n",
        "    print(fake_input_folder)\n",
        "    device, aligned_faces, my_ws=jojo_cuda_DetectResize_nosave(fake_input_folder)\n",
        "    torch.save(aligned_faces, f'{mugshotGAN_folder}{search_image_nu}_aligned_faces.torch', _use_new_zipfile_serialization=False)\n",
        "    torch.save(my_ws, f'{JoJo_folder}{search_image_nu}_my_ws.torch', _use_new_zipfile_serialization=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siJOp2osWt6f"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to reload pytorch checkpoints for jojo face variables in case of crash. { form-width: \"150px\", display-mode: \"form\" }\n",
        "#@markdown You can skip if this is first run.\n",
        "load_checkpoints = True #@param {type:\"boolean\"}\n",
        "if load_checkpoints == True:\n",
        "    my_ws=torch.load(f'{JoJo_folder}{search_image_nu}_my_ws.torch')\n",
        "    aligned_faces=torch.load(f'{mugshotGAN_folder}{search_image_nu}_aligned_faces.torch')\n",
        "    FACES=RESULTS+'{}_FACES/'.format(search_image_nu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgfk78K1ikb8"
      },
      "outputs": [],
      "source": [
        "#@title  { form-width: \"100px\", display-mode: \"form\" }\n",
        "#@title  { display-mode: \"form\" }\n",
        "#@markdown Type style images names into the field bellow without the directory name. Upload multiple style images to do multi-shot image translation\n",
        "#@markdown\n",
        "#@markdown (TICK BOX TO USE CAPI MUGSHOT)\n",
        "%cd {JoJo_folder}\n",
        "use_capis_mugshot = True #@param {type:\"boolean\"}\n",
        "if use_capis_mugshot == True:\n",
        "    #%cd {JoJo_folder+'style_images/'}\n",
        "    !wget -nc https://mint.tokyo/MUGSHOTGAN/yop.png -P {JoJo_folder+'style_images/'}\n",
        "    print('MUGSHOT image ''yop.png'' is ready to use.')\n",
        "    #%cd {JoJo_folder}\n",
        "names = [\"yop.png\"] #@param {type:\"raw\"}\n",
        "\n",
        "targets = []\n",
        "latents = []\n",
        "\n",
        "for name in names:\n",
        "    style_path = os.path.join('style_images', name)\n",
        "    assert os.path.exists(style_path), f\"{style_path} does not exist!\"\n",
        "\n",
        "    name = strip_path_extension(name)\n",
        "\n",
        "    # crop and align the face\n",
        "    style_aligned_path = os.path.join('style_images_aligned', f'{name}.png')\n",
        "    if not os.path.exists(style_aligned_path):\n",
        "        style_aligned = align_face(style_path)\n",
        "        style_aligned.save(style_aligned_path)\n",
        "    else:\n",
        "        style_aligned = Image.open(style_aligned_path).convert('RGB')\n",
        "    #style_path = os.path.join('{}style_images/'.format(JoJo_folder), name)\n",
        "    #style_fix = Image.open(style_path).convert('RGB')\n",
        "    #style_fix.save(style_path)\n",
        "\n",
        "    #assert os.path.exists(style_path), f\"{style_path} does not exist!\"\n",
        "\n",
        "    #name = strip_path_extension(name)\n",
        "\n",
        "    # crop and align the face\n",
        "    #style_aligned_path = os.path.join(JoJo_folder+'style_images_aligned', f'{name}.png')\n",
        "    #if not os.path.exists(style_aligned_path) and style_path.endswith('.png'):\n",
        "     #   style_aligned = Image.open(style_path).convert('RGB')\n",
        "      #  style_aligned.save(style_aligned_path)\n",
        "    #elif not os.path.exists(style_aligned_path):\n",
        "     #   style_aligned = align_face(style_path)\n",
        "      #  style_aligned.save(style_aligned_path)\n",
        "    #else:\n",
        "     #   style_aligned = Image.open(style_aligned_path).convert('RGB')\n",
        "\n",
        "    # GAN invert\n",
        "    style_code_path = os.path.join('inversion_codes', f'{name}.pt')\n",
        "    if not os.path.exists(style_code_path):\n",
        "        latent = e4e_projection(style_aligned, style_code_path, device)\n",
        "    else:\n",
        "        latent = torch.load(style_code_path)['latent']\n",
        "\n",
        "    targets.append(transform(style_aligned).to(device))\n",
        "    latents.append(latent.to(device))\n",
        "\n",
        "targets = torch.stack(targets, 0)\n",
        "latents = torch.stack(latents, 0)\n",
        "\n",
        "target_im = utils.make_grid(targets, normalize=True, range=(-1, 1))\n",
        "#display_image(target_im, title='Style References')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_qNPut_ch3gr"
      },
      "outputs": [],
      "source": [
        "#@title Finetune StyleGAN\n",
        "#@markdown alpha controls the strength of the style\n",
        "alpha =  0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "alpha = 1-alpha\n",
        "\n",
        "#@markdown Tries to preserve color of original image by limiting family of allowable transformations. Set to false if you want to transfer color from reference image. This also leads to heavier stylization\n",
        "preserve_color = True #@param{type:\"boolean\"}\n",
        "#@markdown Number of finetuning steps. Different style reference may require different iterations. Try 200~500 iterations.\n",
        "num_iter =  700#@param {type:\"number\"}\n",
        "#@markdown Log training on wandb and interval for image logging\n",
        "use_wandb = False #@param {type:\"boolean\"}\n",
        "log_interval = 50 #@param {type:\"number\"}\n",
        "\n",
        "latents_temp=latents\n",
        "targets_temp=targets\n",
        "target_im_temp=target_im\n",
        "\n",
        "#print(latents_temp, targets_temp, target_im_temp)\n",
        "\n",
        "if use_wandb:\n",
        "    wandb.init(project=\"JoJoGAN\")\n",
        "    config = wandb.config\n",
        "    config.num_iter = num_iter\n",
        "    config.preserve_color = preserve_color\n",
        "    wandb.log(\n",
        "    {\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im_temp))]},\n",
        "    step=0)\n",
        "\n",
        "# load discriminator for perceptual loss\n",
        "discriminator = Discriminator(1024, 2).eval().to(device)\n",
        "ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
        "discriminator.load_state_dict(ckpt[\"d\"], strict=False)\n",
        "\n",
        "# reset generator\n",
        "del generator\n",
        "generator = deepcopy(original_generator)\n",
        "\n",
        "g_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\n",
        "\n",
        "# Which layers to swap for generating a family of plausible real images -> fake image\n",
        "if preserve_color:\n",
        "    id_swap = [9,11,15,16,17]\n",
        "else:\n",
        "    id_swap = list(range(7, generator.n_latent))\n",
        "\n",
        "for idx in tqdm(range(num_iter)):\n",
        "    mean_w = generator.get_latent(torch.randn([latents_temp.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\n",
        "    in_latent = latents_temp.clone()\n",
        "    in_latent[:, id_swap] = alpha*latents_temp[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\n",
        "\n",
        "    img = generator(in_latent, input_is_latent=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        real_feat = discriminator(targets_temp)\n",
        "    fake_feat = discriminator(img)\n",
        "\n",
        "    loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\n",
        "    \n",
        "    if use_wandb:\n",
        "        wandb.log({\"loss\": loss}, step=idx)\n",
        "        if idx % log_interval == 0:\n",
        "            generator.eval()\n",
        "            my_sample = generator(my_w, input_is_latent=True)\n",
        "            generator.train()\n",
        "            my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\n",
        "            wandb.log(\n",
        "            {\"Current stylization\": [wandb.Image(my_sample)]},\n",
        "            step=idx)\n",
        "\n",
        "    g_optim.zero_grad()\n",
        "    loss.backward()\n",
        "    g_optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJsHJxPsFlBj"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hljMHYp1DQk-"
      },
      "outputs": [],
      "source": [
        "#@title Generate results { form-width: \"100px\", display-mode: \"form\" }\n",
        "n_sample =  1#@param {type:\"number\"}\n",
        "seed = 3000 #@param {type:\"number\"}\n",
        "\n",
        "def JoJoGAN_GENERATOR_one(n_sample=n_sample, seed=seed):\n",
        "\n",
        "#out folders============================================================\n",
        "    JoJoGAN_results=RESULTS+'{}_FAKES/'.format(search_image_nu)  \n",
        "    foldermaker(JoJoGAN_results)\n",
        "\n",
        "#in folder==============================================================\n",
        "    #IMG, total_imgs=list_loader(folder)\n",
        "    total_imgs=int((len(aligned_faces))/2)\n",
        "    print (total_imgs)\n",
        "\n",
        "#load ==================================================================\n",
        "    for f in range (0, total_imgs):\n",
        "        print(f)\n",
        "        aligned_face=aligned_faces[f]\n",
        "        my_w=my_ws[f]\n",
        "        #try:\n",
        "        torch.manual_seed(seed)\n",
        "        with torch.no_grad():\n",
        "            generator.eval()\n",
        "            z = torch.randn(n_sample, latent_dim, device=device)\n",
        "\n",
        "            original_sample = original_generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
        "            sample = generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
        "\n",
        "            original_my_sample = original_generator(my_w, input_is_latent=True)\n",
        "            my_sample = generator(my_w, input_is_latent=True)\n",
        "\n",
        "        # display reference images\n",
        "        style_images = []\n",
        "        for name in names:\n",
        "            style_path =f'style_images_aligned/{strip_path_extension(name)}.png'\n",
        "            style_image = transform(Image.open(style_path).convert('RGB'))\n",
        "            style_images.append(style_image)\n",
        "            \n",
        "        #face = transform(aligned_face).to(device).unsqueeze(0)\n",
        "        #style_images = torch.stack(style_images, 0).to(device)\n",
        "        #display_image(utils.make_grid(style_images, normalize=True, range=(-1, 1)), title='References')\n",
        "\n",
        "        #my_output = torch.cat([face, my_sample], 0)\n",
        "        #display_image(utils.make_grid(my_output, normalize=True, range=(-1, 1)), title='My sample')\n",
        "        producto=utils.make_grid(my_sample, normalize=True, range=(-1, 1))\n",
        "        save_image(producto,JoJoGAN_results+'{}_JOJO_{}.png'.format(search_image_nu, f))\n",
        "\n",
        "        #output = torch.cat([original_sample, sample], 0)\n",
        "        #display_image(utils.make_grid(output, normalize=True, range=(-1, 1), nrow=n_sample), title='Random samples')\n",
        "        f+=1\n",
        "        #except Exception:\n",
        "         #   pass\n",
        "    torch.cuda.empty_cache()\n",
        "JoJoGAN_GENERATOR_one(n_sample, seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z-D7rvkOtIh"
      },
      "outputs": [],
      "source": [
        "#@title Generate results pt 2 for them big bois { form-width: \"100px\", display-mode: \"form\" }\n",
        "\n",
        "def JoJoGAN_GENERATOR_two(n_sample=n_sample, seed=seed):\n",
        "\n",
        "#out folders============================================================\n",
        "    JoJoGAN_results=RESULTS+'{}_FAKES/'.format(search_image_nu) \n",
        "    foldermaker(JoJoGAN_results)\n",
        "\n",
        "#in folder==============================================================\n",
        "    #IMG, total_imgs=list_loader(folder)\n",
        "    total_imgs=int((len(aligned_faces))/2)\n",
        "    print (total_imgs)\n",
        "\n",
        "#load ==================================================================\n",
        "    for f in range (total_imgs, len(aligned_faces)):\n",
        "        print(f)\n",
        "        aligned_face=aligned_faces[f]\n",
        "        my_w=my_ws[f]\n",
        "        #try:\n",
        "        torch.manual_seed(seed)\n",
        "        with torch.no_grad():\n",
        "            generator.eval()\n",
        "            z = torch.randn(n_sample, latent_dim, device=device)\n",
        "\n",
        "            original_sample = original_generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
        "            sample = generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
        "\n",
        "            original_my_sample = original_generator(my_w, input_is_latent=True)\n",
        "            my_sample = generator(my_w, input_is_latent=True)\n",
        "\n",
        "        # display reference images\n",
        "        style_images = []\n",
        "        for name in names:\n",
        "            style_path =f'style_images_aligned/{strip_path_extension(name)}.png'\n",
        "            style_image = transform(Image.open(style_path).convert('RGB'))\n",
        "            style_images.append(style_image)\n",
        "            \n",
        "        #face = transform(aligned_face).to(device).unsqueeze(0)\n",
        "        #style_images = torch.stack(style_images, 0).to(device)\n",
        "        #display_image(utils.make_grid(style_images, normalize=True, range=(-1, 1)), title='References')\n",
        "\n",
        "        #my_output = torch.cat([face, my_sample], 0)\n",
        "        #display_image(utils.make_grid(my_output, normalize=True, range=(-1, 1)), title='My sample')\n",
        "        producto=utils.make_grid(my_sample, normalize=True, range=(-1, 1))\n",
        "        save_image(producto,JoJoGAN_results+'{}_JOJO_{}.png'.format(search_image_nu, f))\n",
        "\n",
        "        #output = torch.cat([original_sample, sample], 0)\n",
        "        #display_image(utils.make_grid(output, normalize=True, range=(-1, 1), nrow=n_sample), title='Random samples')\n",
        "        f+=1\n",
        "    torch.cuda.empty_cache()\n",
        "        #except Exception:\n",
        "         #   pass   \n",
        "JoJoGAN_GENERATOR_two(n_sample, seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8jL8mxDLBELl"
      },
      "outputs": [],
      "source": [
        "#@title Run Real-ESRGAN to upscale the generated image DATASET.\n",
        "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git {RealESRGAN_folder}\n",
        "%cd {RealESRGAN_folder}\n",
        "# Set up the environment\n",
        "!pip install basicsr\n",
        "!pip install facexlib\n",
        "!pip install gfpgan\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "\n",
        "# Download the pre-trained model\n",
        "!wget -nc https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
        "\n",
        "import shutil\n",
        "JOJOfakes=RESULTS+'{}_FAKES/'.format(search_image_nu)\n",
        "os.makedirs('temp_inputs', exist_ok=True) \n",
        "FACES=RESULTS+'{}_FACES/'.format(search_image_nu)\n",
        "RealESRGAN_load_folder = FACES\n",
        "RealESERGAN_upload_folder=RealESRGAN_folder+'temp_inputs/'\n",
        "#!rsync -a {RealESRGAN_load_folder} {RealESRGAN_upload_folder}\n",
        "JOJO_results = True #@param {type:\"boolean\"}\n",
        "if search_image_nu != None and JOJO_results == True:\n",
        "    RealESRGAN_result_folder=RESULTS+search_image_nu+'_jojoFAKE_ESRGAN/'\n",
        "elif search_image_nu != None:\n",
        "    fake_folder=RESULTS+search_image_nu+'_FAKE/'\n",
        "    RealESRGAN_result_folder = RESULTS+search_image_nu+'_FAKE_ESRGAN/'    \n",
        "else:\n",
        "    RealESRGAN_result_folder = RESULTS+search_image+'_ESRGAN/'\n",
        "foldermaker(RealESRGAN_result_folder)\n",
        "realERSGAN_input = JOJOfakes #@param [\"mega_image_folder\", \"FACES\", \"image_folder\", \"None\", \"face_folder\", \"fake_folder\", \"JOJOfakes\"] {type:\"raw\"}\n",
        "!python inference_realesrgan.py -n RealESRGAN_x4plus --input {realERSGAN_input} --outscale 4 --half --face_enhance --output {RealESRGAN_result_folder}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V97Kigcqvhs6"
      },
      "source": [
        "WITH CODE FROM THESE AUTHORS:\n",
        "\n",
        "Pytorch pix2pix by Jun-Yan Zhu @junyanz\n",
        "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
        "\n",
        "JoJoGAN CoLab by Min Jin Chong @ https://github.com/mchong6/JoJoGAN\n",
        "\n",
        "REAL ESRGAN by Xintao @\n",
        "https://github.com/xinntao/Real-ESRGAN.git\n",
        "\n",
        "google-images-download by Hardik Vasa @hardikvasa forked by Joeclinton1 from https://github.com/Joeclinton1/google-images-download.git\n",
        "\n",
        "Xavier Weber \n",
        "https://towardsdatascience.com/deep-learning-based-super-resolution-with-opencv-4fd736678066\n",
        "\n",
        "http://dlib.net/imaging.html#get_frontal_face_detector\n",
        "\n",
        "https://learnopencv.com/edge-detection-using-opencv/\n",
        "\n",
        "@mohanapranes\n",
        "https://www.geeksforgeeks.org/cropping-faces-from-images-using-opencv-python/\n",
        "\n",
        "@misc{sefiks14856,\n",
        "author ={Serengil, Sefik Ilkin},\n",
        "title = { Facial Landmarks for Face Recognition with Dlib },\n",
        "howpublished = { https://sefiks.com/2020/11/20/facial-landmarks-for-face-recognition-with-dlib/ },\n",
        "year = { 2020 },\n",
        "note = \"[Online; accessed 2022-03-19]\"\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "k_sHBJLwE6-s",
        "4imisTxBburL",
        "ZSk3KXlIYhMs",
        "ndYz4aoKcSYX",
        "VZdYrfVcJdqa"
      ],
      "machine_shape": "hm",
      "name": "MUGSHOT_GAN.pynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}